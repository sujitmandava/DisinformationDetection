{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MH9GwuXz8Nl3"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import urllib.request\n",
        "import pandas as pd\n",
        "from urllib.error import HTTPError"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Use the function below to create a dataframe of articles from datasets\n",
        "containing a link to the article.\n",
        "\n",
        "inputFile: Path to the input dataset\n",
        "outputFile: Path for output dataframe to be saved\n",
        "range: Number of articles to be extracted (for testing)\n",
        "'''\n",
        "\n",
        "def extractArticles(inputFile, outputFile, range=-1):\n",
        "  dataset_df = pd.read_csv(inputFile)\n",
        "  df = pd.DataFrame(columns=['Link', 'Article'])\n",
        "  count = 0\n",
        "\n",
        "  for _, row in dataset_df.iterrows():\n",
        "    if range != -1 and count == range:\n",
        "      break\n",
        "    link = str(row['news_url'])\n",
        "    if link[0:4] != 'http':\n",
        "      link = 'https://' + link\n",
        "    count = count + 1\n",
        "    try:\n",
        "      response = urllib.request.urlopen(link, timeout=20)\n",
        "      html = response.read()\n",
        "\n",
        "      soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "      for table in soup.find_all('table'):\n",
        "        table.decompose()\n",
        "      paragraphs = [data.get_text() for data in soup.find_all(\"p\")]\n",
        "      article = ' '.join(paragraphs)\n",
        "      article = ' '.join(article.split())\n",
        "\n",
        "    except HTTPError as e:\n",
        "        if e.code == 404:\n",
        "            article = \"Not found\"\n",
        "        else:\n",
        "            article = f\"Error: {e}\"\n",
        "    except Exception as e:\n",
        "        article = f\"Error: {e}\"\n",
        "    except HTTPError as e:\n",
        "        if e.code == 404:\n",
        "            article = \"Not found\"\n",
        "        else:\n",
        "            article = f\"HTTP Error: {e}\"\n",
        "    except ConnectionError as e:\n",
        "        article = f\"Connection Error: {e}\"\n",
        "    except Exception as e:\n",
        "        article = f\"Error: {e}\"\n",
        "\n",
        "    df = df.append({'Link': link, 'Article': article}, ignore_index=True)\n",
        "\n",
        "  df.to_csv(outputFile, index=False)"
      ],
      "metadata": {
        "id": "8QoZahDZHp3G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}